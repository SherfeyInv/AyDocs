<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[NRI: Hierarchical Representation Learning for Robot Assistants]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2023</AwardEffectiveDate>
<AwardExpirationDate>01/31/2026</AwardExpirationDate>
<AwardTotalIntnAmount>1500000.00</AwardTotalIntnAmount>
<AwardAmount>1186632</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[More than eighteen million people in North America have a physical disability due to limited mobility, restricting their independence, lifestyle, and ability to perform daily activities. One in five older adults struggle with mobility, millions of people with limited mobility are veterans, and a significant number of people have limited mobility because of diseases and accidents. Due to recent advances in artificial intelligence, robots hold great promise to provide timely assistance to people with disabilities, and drive improvements to their quality of life, independence, and productivity. This project introduces an automated robot assistant that is able to recognize a personâ€™s goal, and provide them the right object at the right time, thereby helping people perform complex activities, such as cooking, object repair, and housekeeping. From both sight and dialogue, the research products will be able to anticipate what objects a person will need in the near future, and deliver it at exactly the right moment. Furthermore, the project will generate new educational opportunities at the intersection of robotics, computer vision and natural language processing through a series of systematically designed curriculum and annual capstone projects for assistive robotics.  Due to the tight integration of multiple disciplines and the large practical impact, these educational programs will serve as an excellent platform for training the next generation of roboticists and increasing the diversity in the field. &lt;br/&gt;&lt;br/&gt;This research project introduces a novel hierarchical representation learning framework for assistive robots, which serves as a common interface to drive integration between robotics, computer vision, and natural language understanding. The project includes three thrusts. First, the team will develop hierarchical task representations. Second, human intention prediction and verification will developed. The final thrust will address intention-aware planning. Unlike established state representations in robotics, the new representation leverages non-Euclidean geometry, such as hyperbolic manifolds. Since hyperbolic space is a continuous analog of a tree, it provides new opportunities for learning task hierarchies from large-scale unlabeled instructional videos.   This task representation is able to anticipate the activities of people, steer dialogue to reduce uncertainty, and provide dense rewards for long-horizon planning.  This representation is learned from large-scale unlabeled instructional videos, making this approach flexible and adaptable to the many real-world applications of just-in-time object delivery for people with limited mobility.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>11/24/2023</MinAmdLetterDate>
<MaxAmdLetterDate>12/13/2023</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2405103</AwardID>
<Investigator>
<FirstName>Shuran</FirstName>
<LastName>Song</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shuran Song</PI_FULL_NAME>
<EmailAddress><![CDATA[shuran@stanford.edu]]></EmailAddress>
<NSF_ID>000807731</NSF_ID>
<StartDate>11/24/2023</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>STANFORD</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 JANE STANFORD WAY</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA16</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>HJD6G4D6TJY5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE LELAND STANFORD JUNIOR UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>STANFORD</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052004</ZipCode>
<StreetAddress><![CDATA[450 JANE STANFORD WAY]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>16</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA16</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7484</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<Fund>
<Code>01002223DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2021~1175197</FUND_OBLG>
<FUND_OBLG>2022~11435</FUND_OBLG>
</Award>
</rootTag>
